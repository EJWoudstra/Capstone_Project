{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Science Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook will be mainly used for the Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1 - Introduction to the Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Introduction to Capstone Project </li>\n",
    "<li>Location Data Providers </li>\n",
    "<li>Signing-up for a Watson Studio Account </li>\n",
    "    <li>Peer-review Assignment: Capstone Project Notebook </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading some needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer #HTML parsing\n",
    "import urllib.request #aufrufen von URLs\n",
    "from time import sleep #damit legen wir den Scraper schlafen\n",
    "import json #lesen und schreiben von JSON-Dateien\n",
    "from datetime import datetime #um den Daten Timestamps zu geben\n",
    "import re #regular expressions\n",
    "import os #Dateipfade erstellen und lesen\n",
    "import pandas as pd #Datenanalyse und -manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer #HTML parsing\n",
    "import urllib.request #aufrufen von URLs\n",
    "from time import sleep #damit legen wir den Scraper schlafen\n",
    "import json #lesen und schreiben von JSON-Dateien\n",
    "from datetime import datetime #um den Daten Timestamps zu geben\n",
    "import re #regular expressions\n",
    "import os #Dateipfade erstellen und lesen\n",
    "import pandas as pd #Datenanalyse und -manipulation\n",
    "\n",
    "folders = [\"data/visited/\",\"data/autos/\"]\n",
    "\n",
    "for folder in folders:\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "        print(path, \"erstellt.\")\n",
    "    else:\n",
    "        print(folder,\"existiert bereits\")\n",
    "\n",
    "path_to_visited_urls = \"data/visited/visited_urls.json\"\n",
    "\n",
    "if not os.path.isfile(path_to_visited_urls):\n",
    "    with open(path_to_visited_urls,\"w\") as file:\n",
    "        json.dump([],file)\n",
    "\n",
    "#countries = {\"Deutschland\": \"D\",\n",
    "#             \"Oesterreich\": \"A\",\n",
    "#             \"Belgien\" : \"B\",\n",
    "#             \"Spanien\": \"E\",\n",
    "#             \"Frankreich\": \"F\",\n",
    "#             \"Italien\": \"I\",\n",
    "#             \"Luxemburg\": \"L\",\n",
    "#             \"Niederlande\": \"NL\"}\n",
    "\n",
    "countries = {\"Italien\": \"I\"}\n",
    "\n",
    "car_counter=1\n",
    "cycle_counter=0\n",
    "\n",
    "while True:\n",
    "\n",
    "    with open(path_to_visited_urls) as file:\n",
    "        visited_urls = json.load(file)\n",
    "    \n",
    "    if len(visited_urls) > 100000:\n",
    "        visited_urls = []\n",
    "    \n",
    "    multiple_cars_dict = {}\n",
    "    \n",
    "    cycle_counter+=1\n",
    "\n",
    "    for country in countries:\n",
    "        \n",
    "        car_URLs = []\n",
    "        \n",
    "        for page in range(1,21):\n",
    "\n",
    "            try:\n",
    "                      #https://www.autoscout24.it/lst/fiat/tipo?sort=standard&desc=0&ustate=N%2CU&size=20&page=1&cy=I&fregto=2021&fregfrom=2020&body=5&atype=C&fc=7&qry=&\n",
    "                url = 'https://www.autoscout24.de/lst/fiat/tipo?sort=age&desc=1&ustate=N%2CU&size=20&page='+str(page)+ '&cy=' + countries[country] +'&fregto=2021&fregfrom=2020&body=5&atype=C&fc=7&qry=&'\n",
    "                only_a_tags = SoupStrainer(\"a\")\n",
    "                soup = BeautifulSoup(urllib.request.urlopen(url).read(),'lxml', parse_only=only_a_tags)\n",
    "            except Exception as e:\n",
    "                print(\"Ãœbersicht: \" + str(e) +\" \"*50, end=\"\\r\")\n",
    "                pass\n",
    "\n",
    "            for link in soup.find_all(\"a\"):\n",
    "                if r\"/angebote/\" in str(link.get(\"href\")):\n",
    "                    car_URLs.append(link.get(\"href\"))\n",
    "\n",
    "            car_URLs_unique = [car for car in list(set(car_URLs)) if car not in visited_urls]\n",
    "            \n",
    "            print(f'Lauf {cycle_counter} | {country} | Seite {page} | {len(car_URLs_unique)} neue URLs', end=\"\\r\")\n",
    "        print(\"\")\n",
    "        if len(car_URLs_unique)>0:\n",
    "\n",
    "            for URL in car_URLs_unique:\n",
    "                print(f'Lauf {cycle_counter} | {country} | Auto {car_counter}'+' '*50, end=\"\\r\")\n",
    "                try:\n",
    "                    car_counter+=1\n",
    "\n",
    "                    car_dict = {}\n",
    "                    car_dict[\"country\"] = country\n",
    "                    car_dict[\"date\"] = str(datetime.now())\n",
    "                    print(URL)\n",
    "                    car = BeautifulSoup(urllib.request.urlopen('https://www.autoscout24.de'+URL).read(),'lxml')\n",
    "                    \n",
    "                    for key, value in zip(car.find_all(\"dt\"),car.find_all(\"dd\")):\n",
    "                        car_dict[key.text.replace(\"\\n\",\"\")] = value.text.replace(\"\\n\",\"\")\n",
    "\n",
    "                    car_dict[\"haendler\"] = car.find(\"div\",attrs={\"class\":\"cldt-vendor-contact-box\",\n",
    "                                                                 \"data-vendor-type\":\"dealer\"}) != None\n",
    "\n",
    "                    car_dict[\"privat\"] = car.find(\"div\",attrs={\"class\":\"cldt-vendor-contact-box\",\n",
    "                                                               \"data-vendor-type\":\"privateseller\"}) != None\n",
    "\n",
    "                    car_dict[\"ort\"] = car.find(\"div\",attrs={\"class\":\"sc-grid-col-12\",\n",
    "                                                            \"data-item-name\":\"vendor-contact-city\"}).text\n",
    "                    \n",
    "                    car_dict[\"price\"] =  \"\".join(re.findall(r'[0-9]+',car.find(\"div\",attrs={\"class\":\"cldt-price\"}).text))\n",
    "                    \n",
    "                    ausstattung = []\n",
    "\n",
    "                    for i in car.find_all(\"div\",attrs={\"class\":\"cldt-equipment-block sc-grid-col-3 sc-grid-col-m-4 sc-grid-col-s-12 sc-pull-left\"}):\n",
    "                        for span in i.find_all(\"span\"):\n",
    "                            ausstattung.append(i.text)\n",
    "\n",
    "                    ausstattung2 = []\n",
    "\n",
    "                    for element in list(set(ausstattung)):\n",
    "                        austattung_liste = element.split(\"\\n\")\n",
    "                        ausstattung2.extend(austattung_liste)\n",
    "\n",
    "                    car_dict[\"ausstattung_liste\"] = sorted(list(set(ausstattung2)))\n",
    "\n",
    "                    multiple_cars_dict[URL] = car_dict\n",
    "                    visited_urls.append(URL)\n",
    "                except Exception as e:\n",
    "                    print(\"Detailseite: \" + str(e) + \" \"*50)\n",
    "                    pass\n",
    "            print(\"\")\n",
    "        \n",
    "        else:\n",
    "            print(\"\\U0001F634\")\n",
    "            sleep(60)\n",
    "    \n",
    "    if len(multiple_cars_dict)>0:\n",
    "        df = pd.DataFrame(multiple_cars_dict).T\n",
    "        df.to_csv(\"data/autos/\"+re.sub(\"[.,:,-, ]\",\"_\",str(datetime.now()))+\".csv\",sep=\";\",index_label=\"url\")\n",
    "    else:\n",
    "        print(\"Keine Daten\")\n",
    "    with open(\"data/visited/visited_urls.json\", \"w\") as file:\n",
    "        json.dump(visited_urls, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
